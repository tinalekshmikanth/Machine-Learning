{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tinapraveen/Desktop'"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/Users/tinapraveen/Desktop')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read clean data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>communityname</th>\n",
       "      <th>fold</th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>racePctHisp</th>\n",
       "      <th>agePct12t21</th>\n",
       "      <th>...</th>\n",
       "      <th>PctForeignBorn</th>\n",
       "      <th>PctBornSameState</th>\n",
       "      <th>PctSameHouse85</th>\n",
       "      <th>PctSameCity85</th>\n",
       "      <th>PctSameState85</th>\n",
       "      <th>LandArea</th>\n",
       "      <th>PopDens</th>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <th>LemasPctOfficDrugUn</th>\n",
       "      <th>ViolentCrimesPerPop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alabastercity</td>\n",
       "      <td>7</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.41</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AlexanderCitycity</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Annistoncity</td>\n",
       "      <td>3</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.41</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Athenscity</td>\n",
       "      <td>8</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.39</td>\n",
       "      <td>...</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Auburncity</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   state      communityname  fold  population  householdsize  racepctblack  \\\n",
       "0      1      Alabastercity     7        0.01           0.61          0.21   \n",
       "1      1  AlexanderCitycity    10        0.01           0.41          0.55   \n",
       "2      1       Annistoncity     3        0.03           0.34          0.86   \n",
       "3      1         Athenscity     8        0.01           0.38          0.35   \n",
       "4      1         Auburncity     1        0.04           0.37          0.32   \n",
       "\n",
       "   racePctWhite  racePctAsian  racePctHisp  agePct12t21         ...           \\\n",
       "0          0.83          0.02         0.01         0.41         ...            \n",
       "1          0.57          0.01         0.00         0.47         ...            \n",
       "2          0.30          0.04         0.01         0.41         ...            \n",
       "3          0.71          0.04         0.01         0.39         ...            \n",
       "4          0.70          0.21         0.02         1.00         ...            \n",
       "\n",
       "   PctForeignBorn  PctBornSameState  PctSameHouse85  PctSameCity85  \\\n",
       "0            0.03              0.70            0.40           0.34   \n",
       "1            0.00              0.93            0.66           0.82   \n",
       "2            0.04              0.77            0.59           0.70   \n",
       "3            0.03              0.78            0.56           0.67   \n",
       "4            0.12              0.49            0.12           0.00   \n",
       "\n",
       "   PctSameState85  LandArea  PopDens  PctUsePubTrans  LemasPctOfficDrugUn  \\\n",
       "0            0.57      0.05     0.06            0.01                  0.0   \n",
       "1            0.84      0.11     0.03            0.01                  0.0   \n",
       "2            0.64      0.06     0.11            0.04                  0.0   \n",
       "3            0.71      0.09     0.05            0.00                  0.0   \n",
       "4            0.15      0.09     0.09            0.01                  0.0   \n",
       "\n",
       "   ViolentCrimesPerPop  \n",
       "0                 0.06  \n",
       "1                 0.14  \n",
       "2                 1.00  \n",
       "3                 0.23  \n",
       "4                 0.15  \n",
       "\n",
       "[5 rows x 104 columns]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "clean_df=pd.read_csv('communities-crime-clean.csv')\n",
    "clean_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 a.\tCreate a new field “highCrime” which is true if the crime rate per capita (ViolentCrimesPerPop) is greater than 0.1, and false otherwise. What are the percentage of positive and negative instances in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERCENTAGE OF POSITIVE INSTANCE :  62.7195183141\n",
      "PERCENTAGE OF NEGATIVE INSTANCE :  37.2804816859\n"
     ]
    }
   ],
   "source": [
    "clean_df['highCrime'] =( clean_df['ViolentCrimesPerPop']>0.1).astype(int)\n",
    "clean_df.head()\n",
    "from __future__ import division\n",
    "Percentage_Postiveinst = (sum(clean_df['highCrime'])/len(clean_df)) * 100\n",
    "print(\"PERCENTAGE OF POSITIVE INSTANCE : \", Percentage_Postiveinst)\n",
    "Percentage_Negativeinst = 100 - Percentage_Postiveinst\n",
    "print (\"PERCENTAGE OF NEGATIVE INSTANCE : \" , Percentage_Negativeinst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.b .DECISION TREE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "from sklearn import tree\n",
    "X = clean_df.drop(['communityname','ViolentCrimesPerPop','highCrime','state','fold'],axis=1)\n",
    "Y = clean_df['highCrime']\n",
    "dt_classifier = tree.DecisionTreeClassifier()\n",
    "dt_classifier = dt_classifier.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>racePctHisp</th>\n",
       "      <th>agePct12t21</th>\n",
       "      <th>agePct12t29</th>\n",
       "      <th>agePct16t24</th>\n",
       "      <th>agePct65up</th>\n",
       "      <th>...</th>\n",
       "      <th>NumStreet</th>\n",
       "      <th>PctForeignBorn</th>\n",
       "      <th>PctBornSameState</th>\n",
       "      <th>PctSameHouse85</th>\n",
       "      <th>PctSameCity85</th>\n",
       "      <th>PctSameState85</th>\n",
       "      <th>LandArea</th>\n",
       "      <th>PopDens</th>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <th>LemasPctOfficDrugUn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.57</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.93</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.49</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.04</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   population  householdsize  racepctblack  racePctWhite  racePctAsian  \\\n",
       "0        0.01           0.61          0.21          0.83          0.02   \n",
       "1        0.01           0.41          0.55          0.57          0.01   \n",
       "2        0.03           0.34          0.86          0.30          0.04   \n",
       "3        0.01           0.38          0.35          0.71          0.04   \n",
       "4        0.04           0.37          0.32          0.70          0.21   \n",
       "\n",
       "   racePctHisp  agePct12t21  agePct12t29  agePct16t24  agePct65up  \\\n",
       "0         0.01         0.41         0.49         0.26        0.21   \n",
       "1         0.00         0.47         0.45         0.31        0.57   \n",
       "2         0.01         0.41         0.42         0.27        0.59   \n",
       "3         0.01         0.39         0.46         0.31        0.49   \n",
       "4         0.02         1.00         1.00         1.00        0.14   \n",
       "\n",
       "          ...           NumStreet  PctForeignBorn  PctBornSameState  \\\n",
       "0         ...                0.00            0.03              0.70   \n",
       "1         ...                0.00            0.00              0.93   \n",
       "2         ...                0.02            0.04              0.77   \n",
       "3         ...                0.00            0.03              0.78   \n",
       "4         ...                0.00            0.12              0.49   \n",
       "\n",
       "   PctSameHouse85  PctSameCity85  PctSameState85  LandArea  PopDens  \\\n",
       "0            0.40           0.34            0.57      0.05     0.06   \n",
       "1            0.66           0.82            0.84      0.11     0.03   \n",
       "2            0.59           0.70            0.64      0.06     0.11   \n",
       "3            0.56           0.67            0.71      0.09     0.05   \n",
       "4            0.12           0.00            0.15      0.09     0.09   \n",
       "\n",
       "   PctUsePubTrans  LemasPctOfficDrugUn  \n",
       "0            0.01                  0.0  \n",
       "1            0.01                  0.0  \n",
       "2            0.04                  0.0  \n",
       "3            0.00                  0.0  \n",
       "4            0.01                  0.0  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clean_df.head()\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>DecisionTree_predictions</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highCrime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>743</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "DecisionTree_predictions    0     1\n",
       "highCrime                          \n",
       "0                         743     0\n",
       "1                           0  1250"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_predicted = dt_classifier.predict(X)\n",
    "pd.crosstab(clean_df['highCrime'], dt_predicted, rownames=['highCrime'], colnames=['DecisionTree_predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.b.i) DECISION TREE - ACCURACY , PRECISION AND RECALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  1.0  Precision =  1.0 , Recall = 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score,recall_score,accuracy_score\n",
    "dt_accuracy= accuracy_score(Y, dt_predicted)\n",
    "dt_precision = precision_score( Y, dt_predicted)\n",
    "dt_recall = recall_score( Y, dt_predicted)\n",
    "\n",
    "print( \"Accuracy = \" , dt_accuracy ,\" Precision = \", dt_precision, \", Recall =\" ,dt_recall)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.b.ii)\tWhat are the main features used for classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = dt_classifier.feature_importances_\n",
    "importances = importances[importances!=0]\n",
    "featurenames = np.argsort(importances)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.  PctEmplProfServ 0.360083\n",
      "2.  racePctWhite 0.088817\n",
      "3.  racePctHisp 0.048585\n",
      "4.  indianPerCap 0.023569\n",
      "5.  NumUnderPov 0.019933\n"
     ]
    }
   ],
   "source": [
    "for k in range(5):\n",
    "    print(\"%d.  %s %f\" % (k + 1, X.columns[featurenames[k]], importances[featurenames[k]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.b.ii)Can you explain why they make sense (or not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 5 features are extracted and below given are the descriptions:\n",
    "\n",
    "PctOccupMgmtProf: percentage of people 16 and over who are employed in management or professional occupations\n",
    "racePctWhite: percentage of population that is caucasian\n",
    "racepctHiso: percentage of population that is  Hispanic\n",
    "NumUnderPov number of people under the poverty level\n",
    "blackPerCap: per capita income for african americans \n",
    "\n",
    "Let us analyze the above features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Low Crime rate among those who are employed in professional services: 0.47113055181695823\n",
      "  HighCrime rate among those who are employed in in professional services: 0.4223759999999999\n"
     ]
    }
   ],
   "source": [
    "Lowcrimerate_PctEmplProfServ= clean_df.loc[clean_df['highCrime'] == 0, 'PctEmplProfServ'].mean()\n",
    "#Lowcrimerate_PctEmplProfServ = clean_df.loc[clean_df['highCrime'] == 0, 'Lowcrimerate_PctEmplProfServ'].sum()\n",
    "HighCrimerate_PctEmplProfServ = clean_df.loc[clean_df['highCrime'] == 1, 'PctEmplProfServ'].mean()\n",
    "print(\" Low Crime rate among those who are employed in professional services:\", Lowcrimerate_PctEmplProfServ)\n",
    "print(\"  HighCrime rate among those who are employed in in professional services:\", HighCrimerate_PctEmplProfServ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Low Crime rate among percentage of population that is caucasian: 0.9134589502018869\n",
      " HighCrime rate among percentage of population that is caucasian: 0.6591920000000008\n",
      " Low Crime rate among percentage of population that is  Hispanic: 0.0502691790040378\n",
      " High Crime rate among percentage of population that is  Hispanic : 0.19985599999999976\n",
      " Low Crime rate among  number of people under the poverty level: 0.012893674293405011\n",
      " High Crime rate among number of people under the poverty level : 0.08084000000000016\n",
      " LowCrime rate based on  per capita income for indians : 0.21449528936742934\n",
      " HighCrime rate based on  per capita income for indians  : 0.19707199999999983\n"
     ]
    }
   ],
   "source": [
    "Nocrimerate_racePctWhite= clean_df.loc[clean_df['highCrime'] == 0, 'racePctWhite'].mean()\n",
    "Crimerate_racePctWhite = clean_df.loc[clean_df['highCrime'] == 1, 'racePctWhite'].mean()\n",
    "print(\" Low Crime rate among percentage of population that is caucasian:\", Nocrimerate_racePctWhite)\n",
    "print(\" HighCrime rate among percentage of population that is caucasian:\", Crimerate_racePctWhite)\n",
    "\n",
    "\n",
    "Nocrimerate_racepctHisp= clean_df.loc[clean_df['highCrime'] == 0, 'racePctHisp'].mean()\n",
    "crimerate_racepctHisp = clean_df.loc[clean_df['highCrime'] == 1, 'racePctHisp'].mean()\n",
    "print(\" Low Crime rate among percentage of population that is  Hispanic:\", Nocrimerate_racepctHisp)\n",
    "print(\" High Crime rate among percentage of population that is  Hispanic :\", crimerate_racepctHisp)\n",
    "\n",
    "\n",
    "Nocrimerate_NumUnderPov= clean_df.loc[clean_df['highCrime'] == 0, 'NumUnderPov'].mean()\n",
    "crimerate_NumUnderPov = clean_df.loc[clean_df['highCrime'] == 1, 'NumUnderPov'].mean()\n",
    "print(\" Low Crime rate among  number of people under the poverty level:\", Nocrimerate_NumUnderPov)\n",
    "print(\" High Crime rate among number of people under the poverty level :\", crimerate_NumUnderPov)\n",
    "\n",
    "\n",
    "\n",
    "Nocrimerate_indianPerCap= clean_df.loc[clean_df['highCrime'] == 0, 'indianPerCap'].mean()\n",
    "crimerate_indianPerCap = clean_df.loc[clean_df['highCrime'] == 1, 'indianPerCap'].mean()\n",
    "print(\" LowCrime rate based on  per capita income for indians :\", Nocrimerate_indianPerCap)\n",
    "print(\" HighCrime rate based on  per capita income for indians  :\", crimerate_indianPerCap)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above numbers it can be infered that \n",
    "crime rate is lesser in areas where there are  people who are employed in  professional services. This can be justified as this groupo will be educated and hence chances are less to be engaged in criminal activities than the illiterates.\n",
    "\n",
    "It can be seen that among caucasian the crime rate is less and crime rate is more in case of hispanic populated areas. Crime rate is more where there are people under the poverty line and hence the important features make sense.\n",
    "\n",
    "Based on per capita income of indians NO crime rate is more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.c. i)CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "dt_cv_accuracy = cross_val_score(dt_classifier, X, Y,cv=10,scoring=\"accuracy\")\n",
    "dt_cv_precision = cross_val_score(dt_classifier, X, Y,cv=10,scoring=\"precision\")\n",
    "dt_cv_recall = cross_val_score(dt_classifier, X, Y,cv=10,scoring=\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValidated_Accuracy =  0.732022613065\n",
      "CrossValidated_Precision = 0.78978369709\n",
      "CrossValidated_Recall =  0.7776\n"
     ]
    }
   ],
   "source": [
    "print (\"CrossValidated_Accuracy = \", dt_cv_accuracy.mean())\n",
    "print (\"CrossValidated_Precision =\", dt_cv_precision.mean())\n",
    "print (\"CrossValidated_Recall = \", dt_cv_recall.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.c. ii) Why are they different from the results in the previous test?\n",
    "\n",
    "First we fitted the decision tree classifier on the data which resulted in overfitting and hence accuracy is 1. Now , we have done 10 fold cross validation and which divides the data into 10 folds and leave one out as test fold. This will prevent overfitting of the data and hence we have different values after cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Linear Classification \n",
    "2.a.Gaussian NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy =  0.778223783241 Precision =  0.931623931624 , Recall =  0.6976\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>NB_Predictions</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highCrime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>679</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>378</td>\n",
       "      <td>872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "NB_Predictions    0    1\n",
       "highCrime               \n",
       "0               679   64\n",
       "1               378  872"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb_classifier =GaussianNB()\n",
    "nb_classifier.fit(X,Y)\n",
    "nb_predicted=nb_classifier.predict(X)\n",
    "precision = precision_score( true_values, nb_predicted)\n",
    "recall = recall_score( true_values, nb_predicted)\n",
    "accuracy= accuracy_score( true_values, nb_predicted)\n",
    "print( \" Accuracy = \",accuracy,\"Precision = \",precision,\", Recall = \",recall)\n",
    "pd.crosstab(clean_df['highCrime'], nb_predicted, rownames=['highCrime'], colnames=['NB_Predictions'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.a.i.\tWhat is the 10-fold cross-validation accuracy, precision, and recall for this method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValidated Accuracy_NB : 0.761608040201\n",
      "CrossValidated Precision_NB:  0.911799814828\n",
      "CrossValidated Recall_NB:  0.692\n"
     ]
    }
   ],
   "source": [
    "nb_cv_accuracy = cross_val_score(nb_classifier, X, Y,cv=10,scoring=\"accuracy\")\n",
    "nb_cv_precision = cross_val_score(nb_classifier, X, Y,cv=10,scoring=\"precision\")\n",
    "nb_cv_recall = cross_val_score(nb_classifier, X, Y,cv=10,scoring=\"recall\")\n",
    "print(\"CrossValidated Accuracy_NB :\", nb_cv_accuracy.mean())\n",
    "print(\"CrossValidated Precision_NB: \" ,nb_cv_precision.mean())\n",
    "print(\"CrossValidated Recall_NB: \",  nb_cv_recall.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.a.ii.\tWhat are the 10 most predictive features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>PctKids2Par</td>\n",
       "      <td>0.809336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>PctFam2Par</td>\n",
       "      <td>0.745162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>racePctWhite</td>\n",
       "      <td>0.734884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>PctIlleg</td>\n",
       "      <td>0.708929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>FemalePctDiv</td>\n",
       "      <td>0.693604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>TotalPctDiv</td>\n",
       "      <td>0.674282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>PctYoungKids2Par</td>\n",
       "      <td>0.664671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pctWInvInc</td>\n",
       "      <td>0.660720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>PctTeen2Par</td>\n",
       "      <td>0.642621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>MalePctDivorce</td>\n",
       "      <td>0.616534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Feature     Score\n",
       "46       PctKids2Par  0.809336\n",
       "45        PctFam2Par  0.745162\n",
       "5       racePctWhite  0.734884\n",
       "52          PctIlleg  0.708929\n",
       "42      FemalePctDiv  0.693604\n",
       "43       TotalPctDiv  0.674282\n",
       "47  PctYoungKids2Par  0.664671\n",
       "17        pctWInvInc  0.660720\n",
       "48       PctTeen2Par  0.642621\n",
       "40    MalePctDivorce  0.616534"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_features = pd.DataFrame({\"Feature\": [], \"Score\": []})\n",
    "for k in X.columns:\n",
    "    feature_mean = clean_df.groupby('highCrime')[k].mean()\n",
    "    feature_var = clean_df.groupby('highCrime')[k].var()\n",
    "    feature_sd = sum(np.sqrt(feature_var))\n",
    "    pred_features = pred_features.append({\"Feature\": k, \"Score\": abs(feature_mean[0] - feature_mean[1])/feature_sd}, ignore_index=True)\n",
    "\n",
    "iterations = np.argsort(feature_score['Score'])[::-1]\n",
    "feature_score.iloc[iterations[0:10], ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The larger this different, the more predictive the feature. Why do these make sense ?\n",
    "\n",
    "Higher the normalized mean difference between the positive and negative class, higher the predictive power. Classification accuracy can be improved by defining threshold to classify the instances.\n",
    "\n",
    "2.a.iii) How do these results compare with your results from decision trees?\n",
    "\n",
    "the features which are scored by Naive Bayes are different from the important features from decision tree classifer.\n",
    "\n",
    "Cross Validated  accuracy and Precision  of Guassian NB is more than that of Decision Tree classifier.But recall value of Gaussian NB is lesser than Decision Tree classifier. This means that NB underperforms Decision trees in case of classifying negative classes whereas decision trees have been consistent in classifying positive and negative instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.b.LINEAR SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.850476668339 Precision =  0.883870967742 , Recall =  0.8768\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>SVC_Predictions</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highCrime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>599</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>1096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "SVC_Predictions    0     1\n",
       "highCrime                 \n",
       "0                599   144\n",
       "1                154  1096"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "linearclassifier_SVC=LinearSVC()\n",
    "linearclassifier_SVC.fit(X,Y)\n",
    "svc_predicted=linearclassifier_SVC.predict(X)\n",
    "precision = precision_score( true_values, svc_predicted)\n",
    "recall = recall_score( true_values, svc_predicted)\n",
    "accuracy= accuracy_score( true_values, svc_predicted)\n",
    "print(\"Accuracy = \",accuracy,\"Precision = \",precision, \", Recall = \",recall)\n",
    "pd.crosstab(clean_df['highCrime'], svc_predicted, rownames=['highCrime'], colnames=['SVC_Predictions'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.b.i. What is the 10-fold cross-validation accuracy, precision, and recall for this method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossValidated Accuracy_SVC : 0.796233668342\n",
      "CrossValidated Precision_SVC :  0.845404856531\n",
      "CrossValidated Recall_SVC :  0.8344\n"
     ]
    }
   ],
   "source": [
    "svc_cv_accuracy = cross_val_score(linearclassifier_SVC, X, Y,cv=10,scoring=\"accuracy\")\n",
    "svc_cv_precision = cross_val_score(linearclassifier_SVC, X, Y,cv=10,scoring=\"precision\")\n",
    "svc_cv_recall = cross_val_score(linearclassifier_SVC, X, Y,cv=10,scoring=\"recall\")\n",
    "print(\"CrossValidated Accuracy_SVC :\", svc_cv_accuracy.mean())\n",
    "print(\"CrossValidated Precision_SVC : \" ,svc_cv_precision.mean())\n",
    "print(\"CrossValidated Recall_SVC : \",  svc_cv_recall.mean())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.b.ii.\tWhat are the 10 most predictive features? This can be measured by the absolute feature weights in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.  pctWInvInc 0.257815\n",
      "2.  PersPerOccupHous 0.770301\n",
      "3.  racePctWhite 0.379628\n",
      "4.  PctKids2Par 0.273523\n",
      "5.  RentHighQ 0.333322\n",
      "6.  MalePctDivorce 0.759686\n",
      "7.  NumUnderPov 0.200336\n",
      "8.  NumStreet 0.019634\n",
      "9.  PctOccupMgmtProf 0.404156\n",
      "10.  population 0.009678\n"
     ]
    }
   ],
   "source": [
    "feature_scores_SVC=abs(linearclassifier_SVC.coef_)[0]\n",
    "iterations_SVC = np.argsort(feature_scores_SVC)[::-1]\n",
    "for f in range(10):\n",
    "    print(\"%d.  %s %f\" % (f + 1, X.columns[iterations_SVC[f]], feature_scores_SVC[indices3[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do these make sense (or not)?\n",
    " Let us inspect each of this features and take the mean of crime rate and No crime rate under these features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low Crime rate among Percentage of households with investment / rent income in 1989  0.6204845222072677\n",
      "HighCrime rate among Percentage of households with investment / rent income in 1989  0.42165600000000003\n",
      "Low crime rate in Percentage of population that is caucasian,  0.9134589502018869\n",
      "highCrime rate Percentage of population that is caucasian  0.6591920000000008\n",
      "Low crime rate in Percentage of kids in family housing with two parents:  0.7787617765814268\n",
      "Highcrime rate in Percentage of kids in family housing with two parents:  0.5270159999999994\n"
     ]
    }
   ],
   "source": [
    "Nocrime_pctWInvInc = clean_df.loc[clean_df['highCrime'] == 0, 'pctWInvInc'].mean()\n",
    "Crime_pctWInvInc = clean_df.loc[clean_df['highCrime'] == 1, 'pctWInvInc'].mean()\n",
    "\n",
    "print (\"Low Crime rate among Percentage of households with investment / rent income in 1989 \", Nocrime_pctWInvInc)\n",
    "print (\"HighCrime rate among Percentage of households with investment / rent income in 1989 \", Crime_pctWInvInc)\n",
    "\n",
    "\n",
    "Nocrime_racePctWhite = clean_df.loc[clean_df['highCrime'] == 0, 'racePctWhite'].mean()\n",
    "Crime_racePctWhite = clean_df.loc[clean_df['highCrime'] == 1, 'racePctWhite'].mean()\n",
    "\n",
    "print (\"Low crime rate in Percentage of population that is caucasian, \", Nocrime_racePctWhite)\n",
    "print (\"highCrime rate Percentage of population that is caucasian \", Crime_racePctWhite)\n",
    "\n",
    "\n",
    "\n",
    "Nocrime_PctKids2Par = clean_df.loc[clean_df['highCrime'] == 0, 'PctKids2Par'].mean()\n",
    "Crime_PctKids2Par = clean_df.loc[clean_df['highCrime'] == 1, 'PctKids2Par'].mean()\n",
    "\n",
    "\n",
    "print (\"Low crime rate in Percentage of kids in family housing with two parents: \", Nocrime_PctKids2Par)\n",
    "print (\"Highcrime rate in Percentage of kids in family housing with two parents: \", Crime_PctKids2Par)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from above we can infer that areas where the percentage of households with a higher income are present the crimes are lesser . Crimes are lesser among the families with kids and both parents . Crimes are lesser among caucasian populations as well. Hence the important features make sense."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.b.iii. How do these results compare with your results from decision trees, above?\n",
    "\n",
    "Comparison of Cross validated performance metrics of Decision Tree and SVC\n",
    "Accuracy and precision has imoproved with SVC while recall is nearly the same .\n",
    "Overall SVC performs better in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.a.REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regression_LR=LinearRegression()\n",
    "Y = clean_df['ViolentCrimesPerPop']\n",
    "regression_LR.fit(X,Y)\n",
    "linear_predicted=regression_LR.predict(X)\n",
    "y_true = clean_df['ViolentCrimesPerPop']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.a.ii.\tWhat is the MSE on the training set (train on all the data then test on it all)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE_LR =  0.0165167748803\n"
     ]
    }
   ],
   "source": [
    "MSE=sum((linear_predicted-Y)**2)*(1/len(Y))\n",
    "print(\"MSE_LR = \",MSE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.A.i) \tUsing 10-fold cross-validation, what is the estimated mean-squared-error (MSE) of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE_Cross Validated:  0.0200939693044\n"
     ]
    }
   ],
   "source": [
    "CrossValidated_LR=cross_val_score(regression_LR, X, Y,cv=10,scoring=\"neg_mean_squared_error\")\n",
    "print (\"MSE_Cross Validated: \", abs(CrossValidated_LR.mean()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.a.iii)\tWhat features are most predictive of a high crime rate? A low crime rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.  PctPersOwnOccup 0.675694 False\n",
      "2.  PersPerOccupHous 0.635088 True\n",
      "3.  PctHousOwnOcc 0.568133 True\n",
      "4.  TotalPctDiv 0.561924 False\n",
      "5.  MalePctDivorce 0.458517 True\n",
      "6.  PctRecImmig8 0.432511 True\n",
      "7.  MedRent 0.372728 True\n",
      "8.  whitePerCap 0.351016 False\n",
      "9.  PctKids2Par 0.322651 False\n",
      "10.  OwnOccLowQuart 0.308170 False\n"
     ]
    }
   ],
   "source": [
    "feature_Score_reg = regression_LR.coef_\n",
    "sign_features = feature_Score_reg >= 0\n",
    "feature_Score_reg=abs(regression_LR.coef_)\n",
    "iteration_reg= np.argsort(feature_Score_reg)[::-1]\n",
    "for f in range(10):\n",
    "    print(\"%d.  %s %f %s\" % (f + 1, X.columns[iteration_reg[f]], feature_Score_reg[iteration_reg[f]], sign_features[iteration_reg[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above features impact the low and high crime rates and the regression coefficients are shown as well.\n",
    "\n",
    "Predictive features of low crime are:\n",
    "\n",
    "1.PctPersOwnOccup: percent of people in owner occupied households\n",
    "\n",
    "2.TotalPctDiv: percentage of population who are divorced\n",
    "\n",
    "\n",
    "3.OwnOccLowQuart:owner occupied housing - lower quartile value percentage of males who are divorced\n",
    "\n",
    "4.whitePerCap: per capita income for caucasians\n",
    "\n",
    "5.PctKids2Par:percentage of kids in family housing with two parents\n",
    "\n",
    "Predictive features of high crime are:\n",
    "\n",
    "1.PctHousOwnOcc: percent of households owner occupied\n",
    "\n",
    "2.PersPerOccupHous: mean persons per household\n",
    "\n",
    "3.MalePctDivorce :percentage of males who are divorced\n",
    "\n",
    "\n",
    "4.PctRecImmig8 : percentage of _immigrants_ who immigated within last 8 year\n",
    "\n",
    "\n",
    "5.MedRent: median gross rent "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.b.RIDGE REGRESSION\n",
    "3.b.i)\tWhat is the estimated MSE of the model under 10-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE_Cross Validated =  0.0167635291552\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "regression_RIDGE_cv = RidgeCV(alphas=(10,1,0.1,0.01,0.001),cv=10)\n",
    "regression_RIDGE_cv.fit(X,Y)\n",
    "ridge_cv_predicted=regression_RIDGE_cv.predict(X)\n",
    "MSE=sum((ridge_cv_predicted-Y)**2)*(1/len(Y))\n",
    "print(\"MSE_Cross Validated = \",MSE)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.b.ii) What is the MSE on the training set (train on all the data then test on it all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE wio CV =  0.0167635291552\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "regression_RIDGE = Ridge(alpha= 1)\n",
    "regression_RIDGE.fit(X,Y)\n",
    "ridge_predicted=regression_RIDGE.predict(X)\n",
    "MSE=sum((ridge_predicted-Y)**2)*(1/len(Y))\n",
    "print(\"MSE wio CV = \",MSE)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.b.iii.\tWhat is the best alpha?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Alpha =  1\n"
     ]
    }
   ],
   "source": [
    "bestAlpha=regression_RIDGE_cv.alpha_\n",
    "print(\"Best Alpha = \",bestAlpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.b.iv) .  What does this say about the amount of overfitting in linear regression for this problem?\n",
    "\n",
    "The results, i.e., MSE with and without cross validation are the same  in case of ridge . NO significant change in MSE.\n",
    "But cross validated MSE on linear regression is 0.019. On comparing with linear regression and ridge regression , MSE on ridge is lesser which means that overfitting is less in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.c.Quadratic Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.00000000e+00   1.00000000e-02   6.10000000e-01 ...,   1.00000000e-04\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  1.00000000e+00   1.00000000e-02   4.10000000e-01 ...,   1.00000000e-04\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  1.00000000e+00   3.00000000e-02   3.40000000e-01 ...,   1.60000000e-03\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " ..., \n",
      " [  1.00000000e+00   3.00000000e-02   4.00000000e-01 ...,   4.00000000e-04\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  1.00000000e+00   1.00000000e-02   4.50000000e-01 ...,   1.02400000e-01\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  1.00000000e+00   1.00000000e-02   2.90000000e-01 ...,   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "Quad_regression = PolynomialFeatures(degree=2)\n",
    "quadratic = Quad_regression.fit_transform(X)\n",
    "print(quadratic)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.c.i) \tWhat is the estimated MSE of the model under 10-fold CV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE_Cross Validated = :  0.129898436576\n"
     ]
    }
   ],
   "source": [
    "quad_model=LinearRegression()\n",
    "quad_cv=cross_val_score(quad_model, quadratic, Y,cv=10,scoring=\"neg_mean_squared_error\")\n",
    "print(\"MSE_Cross Validated = : \", abs(quad_cv.mean()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.c.ii.\tWhat is the MSE on the training set (train on all the data then test on it all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE wio cv =  1.36976560371e-28\n"
     ]
    }
   ],
   "source": [
    "quad_model.fit(quadratic,Y)\n",
    "quad_predicted=quad_model.predict(quadratic)\n",
    "MSE=sum((quad_predicted-Y)**2)*(1/len(Y))\n",
    "print(\"MSE wio cv = \",MSE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.c.iii.Does this mean the quadratic model is better than the linear model for this problem?\n",
    "\n",
    "Cross Validated MSE of linear regression is lesser than the quadratic model. So the linear model is better for this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Dirty Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>county</th>\n",
       "      <th>community</th>\n",
       "      <th>communityname</th>\n",
       "      <th>fold</th>\n",
       "      <th>population</th>\n",
       "      <th>householdsize</th>\n",
       "      <th>racepctblack</th>\n",
       "      <th>racePctWhite</th>\n",
       "      <th>racePctAsian</th>\n",
       "      <th>...</th>\n",
       "      <th>LandArea</th>\n",
       "      <th>PopDens</th>\n",
       "      <th>PctUsePubTrans</th>\n",
       "      <th>PolicCars</th>\n",
       "      <th>PolicOperBudg</th>\n",
       "      <th>LemasPctPolicOnPatr</th>\n",
       "      <th>LemasGangUnitDeploy</th>\n",
       "      <th>LemasPctOfficDrugUn</th>\n",
       "      <th>PolicBudgPerPop</th>\n",
       "      <th>ViolentCrimesPerPop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>Lakewoodcity</td>\n",
       "      <td>1</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>Tukwilacity</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.45</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>Aberdeentown</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.02</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>81440</td>\n",
       "      <td>Willingborotownship</td>\n",
       "      <td>1</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.77</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.28</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>95</td>\n",
       "      <td>6096</td>\n",
       "      <td>Bethlehemtownship</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.02</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>?</td>\n",
       "      <td>0.00</td>\n",
       "      <td>?</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   state county community        communityname  fold  population  \\\n",
       "0      8      ?         ?         Lakewoodcity     1        0.19   \n",
       "1     53      ?         ?          Tukwilacity     1        0.00   \n",
       "2     24      ?         ?         Aberdeentown     1        0.00   \n",
       "3     34      5     81440  Willingborotownship     1        0.04   \n",
       "4     42     95      6096    Bethlehemtownship     1        0.01   \n",
       "\n",
       "   householdsize  racepctblack  racePctWhite  racePctAsian  \\\n",
       "0           0.33          0.02          0.90          0.12   \n",
       "1           0.16          0.12          0.74          0.45   \n",
       "2           0.42          0.49          0.56          0.17   \n",
       "3           0.77          1.00          0.08          0.12   \n",
       "4           0.55          0.02          0.95          0.09   \n",
       "\n",
       "          ...           LandArea  PopDens  PctUsePubTrans  PolicCars  \\\n",
       "0         ...               0.12     0.26            0.20       0.06   \n",
       "1         ...               0.02     0.12            0.45          ?   \n",
       "2         ...               0.01     0.21            0.02          ?   \n",
       "3         ...               0.02     0.39            0.28          ?   \n",
       "4         ...               0.04     0.09            0.02          ?   \n",
       "\n",
       "   PolicOperBudg  LemasPctPolicOnPatr  LemasGangUnitDeploy  \\\n",
       "0           0.04                  0.9                  0.5   \n",
       "1              ?                    ?                    ?   \n",
       "2              ?                    ?                    ?   \n",
       "3              ?                    ?                    ?   \n",
       "4              ?                    ?                    ?   \n",
       "\n",
       "   LemasPctOfficDrugUn  PolicBudgPerPop  ViolentCrimesPerPop  \n",
       "0                 0.32             0.14                 0.20  \n",
       "1                 0.00                ?                 0.67  \n",
       "2                 0.00                ?                 0.43  \n",
       "3                 0.00                ?                 0.12  \n",
       "4                 0.00                ?                 0.03  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dirty = pd.read_csv('communities-crime-full.csv')\n",
    "df_dirty.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      state  county  community  fold  population  householdsize  racepctblack  \\\n",
      "0         8      58      46188     1           0              0             0   \n",
      "1        53      58      46188     1           0              0             0   \n",
      "2        24      58      46188     1           0              0             0   \n",
      "3        34       5      81440     1           0              0             1   \n",
      "4        42      95       6096     1           0              0             0   \n",
      "5         6      58      46188     1           0              0             0   \n",
      "6        44       7      41500     1           0              0             0   \n",
      "7         6      58      46188     1           0              0             0   \n",
      "8        21      58      46188     1           0              0             0   \n",
      "9        29      58      46188     1           0              0             0   \n",
      "10        6      58      46188     1           0              0             0   \n",
      "11       36      58      46188     1           0              0             0   \n",
      "12       25      21      44105     1           0              0             0   \n",
      "13       55      87      30075     1           0              0             0   \n",
      "14        6      58      46188     1           0              0             0   \n",
      "15       19     187      91370     1           0              0             0   \n",
      "16       36       1       1000     1           0              0             0   \n",
      "17       34      27      17650     1           0              0             0   \n",
      "18       18      58      46188     1           0              0             0   \n",
      "19       42     129      66376     1           0              0             0   \n",
      "20        6      58      46188     1           0              0             0   \n",
      "21       12      31      46188     1           1              0             0   \n",
      "22       41      58      46188     1           0              0             0   \n",
      "23       19     193      93926     1           0              0             0   \n",
      "24        6      58      46188     1           0              0             0   \n",
      "25        8      58      46188     1           0              0             0   \n",
      "26        6      58      46188     1           0              0             0   \n",
      "27       39      29      61798     1           0              0             0   \n",
      "28       54      58      46188     1           0              0             0   \n",
      "29        9       7      22490     1           0              0             0   \n",
      "...     ...     ...        ...   ...         ...            ...           ...   \n",
      "1964     36     119      49121    10           0              0             1   \n",
      "1965     18      97      46188    10           1              0             0   \n",
      "1966      6      58      46188    10           0              0             0   \n",
      "1967     25      17      76135    10           0              0             0   \n",
      "1968     34      25      62430    10           0              0             0   \n",
      "1969      6      58      46188    10           1              0             0   \n",
      "1970     45      58      46188    10           0              0             0   \n",
      "1971      1      58      46188    10           0              0             0   \n",
      "1972      6      58      46188    10           0              0             0   \n",
      "1973     39      58      46188    10           0              0             0   \n",
      "1974     37      58      46188    10           0              0             0   \n",
      "1975     53      58      46188    10           0              0             0   \n",
      "1976     25      17       1605    10           0              0             0   \n",
      "1977     47      58      46188    10           0              0             0   \n",
      "1978      9       1       4720    10           0              0             0   \n",
      "1979     34      39      61530    10           0              0             0   \n",
      "1980     25       5      69170    10           0              0             0   \n",
      "1981      9       9      35650    10           0              0             0   \n",
      "1982     42     133      73168    10           0              0             0   \n",
      "1983     13      58      46188    10           0              0             0   \n",
      "1984     39      95      48342    10           0              0             0   \n",
      "1985      1      58      46188    10           0              0             0   \n",
      "1986      9       3      70550    10           0              0             0   \n",
      "1987     44       7      11800    10           0              0             0   \n",
      "1988     28      58      46188    10           0              0             1   \n",
      "1989     12      58      46188    10           0              0             0   \n",
      "1990      6      58      46188    10           0              0             0   \n",
      "1991      9       9      80070    10           0              0             0   \n",
      "1992     25      17      72600    10           0              0             0   \n",
      "1993      6      58      46188    10           0              0             0   \n",
      "\n",
      "      racePctWhite  racePctAsian  racePctHisp       ...         \\\n",
      "0                0             0            0       ...          \n",
      "1                0             0            0       ...          \n",
      "2                0             0            0       ...          \n",
      "3                0             0            0       ...          \n",
      "4                0             0            0       ...          \n",
      "5                0             1            0       ...          \n",
      "6                0             0            0       ...          \n",
      "7                0             0            1       ...          \n",
      "8                0             0            0       ...          \n",
      "9                0             0            0       ...          \n",
      "10               0             1            0       ...          \n",
      "11               0             0            0       ...          \n",
      "12               0             0            0       ...          \n",
      "13               0             0            0       ...          \n",
      "14               0             0            0       ...          \n",
      "15               0             0            0       ...          \n",
      "16               0             0            0       ...          \n",
      "17               0             0            0       ...          \n",
      "18               0             0            0       ...          \n",
      "19               0             0            0       ...          \n",
      "20               0             0            0       ...          \n",
      "21               0             0            0       ...          \n",
      "22               0             0            0       ...          \n",
      "23               0             0            0       ...          \n",
      "24               0             1            1       ...          \n",
      "25               0             0            0       ...          \n",
      "26               0             1            0       ...          \n",
      "27               0             0            0       ...          \n",
      "28               0             0            0       ...          \n",
      "29               0             0            0       ...          \n",
      "...            ...           ...          ...       ...          \n",
      "1964             0             0            0       ...          \n",
      "1965             0             0            0       ...          \n",
      "1966             0             0            0       ...          \n",
      "1967             0             0            0       ...          \n",
      "1968             0             0            0       ...          \n",
      "1969             0             1            0       ...          \n",
      "1970             0             0            0       ...          \n",
      "1971             0             0            0       ...          \n",
      "1972             0             0            0       ...          \n",
      "1973             0             0            0       ...          \n",
      "1974             0             0            0       ...          \n",
      "1975             0             0            0       ...          \n",
      "1976             0             0            0       ...          \n",
      "1977             0             0            0       ...          \n",
      "1978             0             0            0       ...          \n",
      "1979             0             0            0       ...          \n",
      "1980             0             0            0       ...          \n",
      "1981             0             0            0       ...          \n",
      "1982             0             0            0       ...          \n",
      "1983             0             0            0       ...          \n",
      "1984             0             0            0       ...          \n",
      "1985             0             0            0       ...          \n",
      "1986             0             0            0       ...          \n",
      "1987             1             0            0       ...          \n",
      "1988             0             0            0       ...          \n",
      "1989             0             0            0       ...          \n",
      "1990             0             0            0       ...          \n",
      "1991             0             0            0       ...          \n",
      "1992             0             0            0       ...          \n",
      "1993             0             0            0       ...          \n",
      "\n",
      "      PolicAveOTWorked  LandArea  PopDens  PctUsePubTrans  PolicCars  \\\n",
      "0                    0         0        0               0          0   \n",
      "1                    0         0        0               0          0   \n",
      "2                    0         0        0               0          0   \n",
      "3                    0         0        0               0          0   \n",
      "4                    0         0        0               0          0   \n",
      "5                    0         0        0               0          0   \n",
      "6                    0         0        0               0          0   \n",
      "7                    0         0        0               0          0   \n",
      "8                    0         0        0               0          0   \n",
      "9                    0         0        0               0          0   \n",
      "10                   0         0        1               1          0   \n",
      "11                   0         0        0               1          0   \n",
      "12                   0         0        0               0          0   \n",
      "13                   0         0        0               0          0   \n",
      "14                   0         0        0               0          0   \n",
      "15                   0         0        0               0          0   \n",
      "16                   0         0        0               0          0   \n",
      "17                   0         0        0               0          0   \n",
      "18                   0         0        0               0          0   \n",
      "19                   0         0        0               0          0   \n",
      "20                   0         0        0               0          0   \n",
      "21                   1         1        0               0          1   \n",
      "22                   0         0        0               0          0   \n",
      "23                   0         0        0               0          0   \n",
      "24                   0         0        0               0          0   \n",
      "25                   0         0        0               0          0   \n",
      "26                   0         0        0               0          0   \n",
      "27                   0         0        0               0          0   \n",
      "28                   0         0        0               0          0   \n",
      "29                   0         0        0               0          0   \n",
      "...                ...       ...      ...             ...        ...   \n",
      "1964                 0         0        1               1          0   \n",
      "1965                 0         1        0               0          0   \n",
      "1966                 0         0        0               0          0   \n",
      "1967                 0         0        0               0          0   \n",
      "1968                 0         0        0               0          0   \n",
      "1969                 0         0        1               1          0   \n",
      "1970                 0         0        0               0          0   \n",
      "1971                 0         0        0               0          0   \n",
      "1972                 0         0        0               0          0   \n",
      "1973                 0         0        0               0          0   \n",
      "1974                 0         0        0               0          0   \n",
      "1975                 0         0        0               0          0   \n",
      "1976                 0         0        0               0          0   \n",
      "1977                 0         0        0               0          0   \n",
      "1978                 0         0        0               0          0   \n",
      "1979                 0         0        0               0          0   \n",
      "1980                 0         0        0               0          0   \n",
      "1981                 0         0        0               0          0   \n",
      "1982                 0         0        0               0          0   \n",
      "1983                 0         0        0               0          0   \n",
      "1984                 0         0        0               0          0   \n",
      "1985                 0         0        0               0          0   \n",
      "1986                 0         0        0               0          0   \n",
      "1987                 0         0        0               0          0   \n",
      "1988                 0         0        0               0          0   \n",
      "1989                 0         0        0               0          0   \n",
      "1990                 0         0        0               0          0   \n",
      "1991                 0         0        0               0          0   \n",
      "1992                 0         0        0               0          0   \n",
      "1993                 0         0        0               0          0   \n",
      "\n",
      "      PolicOperBudg  LemasPctPolicOnPatr  LemasGangUnitDeploy  \\\n",
      "0                 0                    0                    0   \n",
      "1                 0                    0                    0   \n",
      "2                 0                    0                    0   \n",
      "3                 0                    0                    0   \n",
      "4                 0                    0                    0   \n",
      "5                 0                    0                    0   \n",
      "6                 0                    0                    0   \n",
      "7                 0                    0                    0   \n",
      "8                 0                    0                    0   \n",
      "9                 0                    0                    0   \n",
      "10                0                    0                    0   \n",
      "11                0                    0                    0   \n",
      "12                0                    0                    0   \n",
      "13                0                    0                    0   \n",
      "14                0                    0                    0   \n",
      "15                0                    0                    0   \n",
      "16                0                    0                    0   \n",
      "17                0                    0                    0   \n",
      "18                0                    0                    0   \n",
      "19                0                    0                    0   \n",
      "20                0                    0                    0   \n",
      "21                0                    0                    0   \n",
      "22                0                    0                    0   \n",
      "23                0                    0                    1   \n",
      "24                0                    0                    0   \n",
      "25                0                    0                    0   \n",
      "26                0                    0                    0   \n",
      "27                0                    0                    0   \n",
      "28                0                    0                    0   \n",
      "29                0                    0                    0   \n",
      "...             ...                  ...                  ...   \n",
      "1964              0                    0                    0   \n",
      "1965              0                    0                    0   \n",
      "1966              0                    0                    0   \n",
      "1967              0                    0                    0   \n",
      "1968              0                    0                    0   \n",
      "1969              0                    0                    0   \n",
      "1970              0                    0                    0   \n",
      "1971              0                    0                    0   \n",
      "1972              0                    0                    0   \n",
      "1973              0                    0                    0   \n",
      "1974              0                    0                    0   \n",
      "1975              0                    0                    0   \n",
      "1976              0                    0                    0   \n",
      "1977              0                    0                    0   \n",
      "1978              0                    0                    0   \n",
      "1979              0                    0                    0   \n",
      "1980              0                    0                    0   \n",
      "1981              0                    0                    0   \n",
      "1982              0                    0                    0   \n",
      "1983              0                    0                    0   \n",
      "1984              0                    0                    0   \n",
      "1985              0                    0                    0   \n",
      "1986              0                    0                    0   \n",
      "1987              0                    0                    0   \n",
      "1988              0                    0                    0   \n",
      "1989              0                    0                    0   \n",
      "1990              0                    0                    0   \n",
      "1991              0                    0                    0   \n",
      "1992              0                    0                    0   \n",
      "1993              0                    0                    0   \n",
      "\n",
      "      LemasPctOfficDrugUn  PolicBudgPerPop  \n",
      "0                       0                0  \n",
      "1                       0                0  \n",
      "2                       0                0  \n",
      "3                       0                0  \n",
      "4                       0                0  \n",
      "5                       0                0  \n",
      "6                       0                0  \n",
      "7                       0                0  \n",
      "8                       0                0  \n",
      "9                       0                0  \n",
      "10                      0                0  \n",
      "11                      0                0  \n",
      "12                      0                0  \n",
      "13                      0                0  \n",
      "14                      0                0  \n",
      "15                      0                0  \n",
      "16                      0                0  \n",
      "17                      0                0  \n",
      "18                      0                0  \n",
      "19                      0                0  \n",
      "20                      0                0  \n",
      "21                      0                0  \n",
      "22                      0                0  \n",
      "23                      0                0  \n",
      "24                      0                0  \n",
      "25                      0                0  \n",
      "26                      0                0  \n",
      "27                      0                0  \n",
      "28                      0                0  \n",
      "29                      0                0  \n",
      "...                   ...              ...  \n",
      "1964                    0                0  \n",
      "1965                    0                0  \n",
      "1966                    0                0  \n",
      "1967                    0                0  \n",
      "1968                    0                0  \n",
      "1969                    0                0  \n",
      "1970                    0                0  \n",
      "1971                    0                0  \n",
      "1972                    0                0  \n",
      "1973                    0                0  \n",
      "1974                    0                0  \n",
      "1975                    0                0  \n",
      "1976                    0                0  \n",
      "1977                    0                0  \n",
      "1978                    0                0  \n",
      "1979                    0                0  \n",
      "1980                    0                0  \n",
      "1981                    0                0  \n",
      "1982                    0                0  \n",
      "1983                    0                0  \n",
      "1984                    0                0  \n",
      "1985                    0                0  \n",
      "1986                    0                0  \n",
      "1987                    0                0  \n",
      "1988                    0                0  \n",
      "1989                    0                0  \n",
      "1990                    0                0  \n",
      "1991                    0                0  \n",
      "1992                    0                0  \n",
      "1993                    1                0  \n",
      "\n",
      "[1994 rows x 126 columns]\n"
     ]
    }
   ],
   "source": [
    "df_dirty['highCrime'] = (df_dirty['ViolentCrimesPerPop'] > 0.1).astype(int)\n",
    "X = df_dirty.drop(['communityname','ViolentCrimesPerPop','highCrime'],axis=1)\n",
    "X=X.apply(pd.to_numeric,errors='coerce')\n",
    "X = X.fillna(X.mean()).astype(int).astype(int)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted_dirtyData</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highCrime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>707</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40</td>\n",
       "      <td>1211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted_dirtyData    0     1\n",
       "highCrime                     \n",
       "0                    707    36\n",
       "1                     40  1211"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = df_dirty['highCrime']\n",
    "dt_dirty = tree.DecisionTreeClassifier()\n",
    "dt_dirty = dt_dirty.fit(X, Y)\n",
    "dt_dirty_predicted = dt_dirty.predict(X)\n",
    "pd.crosstab(df_dirty['highCrime'], dt_dirty_predicted, rownames=['highCrime'], colnames=['Predicted_dirtyData'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.961885656971 Precision =  0.971130713713 , Recall =  0.968025579536\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score,recall_score,accuracy_score\n",
    "y_true = df_dirty['highCrime']\n",
    "accuracy= accuracy_score( y_true, dt_dirty_predicted )\n",
    "precision = precision_score( y_true, dt_dirty_predicted )\n",
    "recall = recall_score( y_true, dt_dirty_predicted )\n",
    "print(\"Accuracy = \",accuracy,\"Precision = \",precision,\", Recall = \",recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy_CV =  0.670583552089\n",
      "Precision_CV =  0.736195174239\n",
      "Recall_CV =  0.726685714286\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracy_cv = cross_val_score(dt_dirty, X, Y,cv=10,scoring=\"accuracy\")\n",
    "precision_cv= cross_val_score(dt_dirty, X, Y,cv=10,scoring=\"precision\")\n",
    "recall_cv = cross_val_score(dt_dirty, X, Y,cv=10,scoring=\"recall\")\n",
    "print (\"Accuracy_CV = \", accuracy_cv.mean())\n",
    "print (\"Precision_CV = \", precision_cv.mean())\n",
    "print (\"Recall_CV = \", recall_cv.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.a.\tAre the CV results better or worse? What does this say about the effect of missing values?\n",
    "\n",
    "Cross Validated Accuracy for Decision Tree Classifier in clean data set is 0.71 whereas in the dirty data set it is 0.66 and so are the precision(0.778473506115 and  0.737086806445 respectively ) and recall(0.7784 and 0.729079365079 respectively )  values\n",
    "it can be seen that the values are lesser in case of dirty data set. Hence it can be concluded that the presence of missing values will have an impact on the performance metrics of any algorithms.\n",
    "Though decision tree classfiers are comparitively resisitant to effect of missing values ,it can be seen that the metrics are affected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
